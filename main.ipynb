{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAy44E3sgJ_z"
      },
      "source": [
        "## **Input experiment parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MBTAww4Heflt",
        "outputId": "0ab803a8-bcea-44c1-dc89-b2d4856e6093"
      },
      "source": [
        "experiement_string = \"DQN\"\n",
        "'''This is where the algorithm is decided for the experiement parameters\n",
        "Allowing experiement_strings are :\n",
        "\"DQN\"\n",
        "“DQN+PER”\n",
        "“DDQN”\n",
        "“DDQN+PER”\n",
        "“DDQN+D2QN”\n",
        "“DDQN+D2QN+PER”\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This is where the algorithm is decided for the experiement parameters\\nAllowing experiement_strings are :\\n\"DQN\"\\n“DQN+PER”\\n“DDQN”\\n“DDQN+PER”\\n“DDQN+D2QN”\\n“DDQN+D2QN+PER”\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bmg0--6ChF5R",
        "outputId": "bee44f2e-dc78-44be-e8e1-7370eb3fea4c"
      },
      "source": [
        "save_name = \"DDQN+D2QN+PER → DDQN+PER → DDQN+PER\"\n",
        "'''This is the DNN save name for automatic saving to Google Drive'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This is the DNN save name for automatic saving to Google Drive'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uc53_Fy2mJPT",
        "outputId": "813438c2-99e9-426a-c32f-f43463ace3d5"
      },
      "source": [
        "transfer = False\n",
        "''' Set transfer to False to enable a blank run and too True if you wish to transfer a model'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Set transfer to False to enable a blank run and too True if you wish to transfer a model'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9e9YRYFMuBca",
        "outputId": "5723ac24-8145-4cab-aabd-9c90758d84e4"
      },
      "source": [
        "tranfser_file = \"DDQN+D2QN+PER → DDQN+PER\"\n",
        "''' Set the file name for the DNN you wish to transfer to a new algorithm'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Set the file name for the DNN you wish to transfer to a new algorithm'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "f0Z8Ahs-t8-C",
        "outputId": "0d1e14f6-2390-4737-b3f2-cd4a7134738b"
      },
      "source": [
        "transfer_type = \"DQN→DQN\"\n",
        "''' Set the type of transfer learning experiement\n",
        "Allowed options are:\n",
        "\"DQN→DQN\"\n",
        "\"D2QN→DQN\"\n",
        "\"DQN→D2QN\"\n",
        "\"D2QN→D2QN\"\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Set the type of transfer learning experiement\\nAllowed options are:\\n\"DQN→DQN\"\\n\"D2QN→DQN\"\\n\"DQN→D2QN\"\\n\"D2QN→D2QN\"\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5Lf5T4O9W3X"
      },
      "source": [
        "# 128 eval parm\n",
        "do_hundrad_twenty_eight_evaluation_num = 25\n",
        "save_max_tiles = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tO2JsEgxuQx",
        "outputId": "7697bef5-abcc-4b6c-f6dc-8d8cb5e51dfa"
      },
      "source": [
        "'''Connect to drive'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlIFidIQvfDN"
      },
      "source": [
        "## **Train agent logic**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr4goDcXJYRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b2964f3-c861-4ed0-a4bf-f536461f59e1"
      },
      "source": [
        "!pip install cpprb \n",
        "!pip install tabulate "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cpprb in /usr/local/lib/python3.7/dist-packages (10.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from cpprb) (1.19.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (0.8.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDLEeirC8J0s"
      },
      "source": [
        "from cpprb import ReplayBuffer,PrioritizedReplayBuffer\n",
        "import cpprb\n",
        "from tabulate import tabulate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN_Xi_EonKDv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "import sys\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# import gym\n",
        "\n",
        "from time import ctime\n",
        "import time\n",
        "# import gym_2048\n",
        "import sys\n",
        "import pickle\n",
        "import numpy as np\n",
        "import math\n",
        "from numba.experimental import jitclass\n",
        "from numba import jit\n",
        "from numba import njit, gdb_init\n",
        "from numba import uint32, int32, int32, b1, float64, float32\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6m-gQk3JW7t"
      },
      "source": [
        "class pickle_class:\n",
        "    def __init__(self,episode,steps,tiles,board_sum,score):\n",
        "        self.episode = episode\n",
        "        self.steps = steps\n",
        "        self.tiles = tiles\n",
        "        self.board_sum = board_sum\n",
        "        self.score = score\n",
        "\n",
        "\n",
        "def check_board_update(state,next_state,env):\n",
        "\n",
        "    if np.array_equal(state,next_state):\n",
        "\n",
        "        next_state_flat = next_state.flatten()\n",
        "\n",
        "\n",
        "        smallest_index = np.where(next_state_flat==np.min(next_state_flat))\n",
        "  \n",
        "        random_index = np.random.choice(smallest_index[0])\n",
        "\n",
        "\n",
        "        if np.random.uniform(0,1) > 0.1:\n",
        "\n",
        "            next_state_flat[random_index] = 2\n",
        "\n",
        "        else:\n",
        "            next_state_flat[random_index] = 4\n",
        "\n",
        "        next_state = next_state_flat.reshape(4,-1)\n",
        "     \n",
        "        env.set_board(next_state)\n",
        "  \n",
        "    return next_state,env\n",
        "\n",
        "def normalise_training_samples(states,next_states,rewards):\n",
        "\n",
        "  # normalise state\n",
        "  normalised_states = np.array([],dtype=np.float32)\n",
        "\n",
        "  for board in states:\n",
        "\n",
        "    normalised_board = np.array([],dtype=np.float32)\n",
        "\n",
        "    for tile in board:\n",
        "\n",
        "      if np.array_equal(tile,0):\n",
        "\n",
        "        normalised_board = np.append(normalised_board,0)\n",
        "\n",
        "      else:\n",
        "      \n",
        "        normalised_board = np.append(normalised_board,np.log2(tile)/7)\n",
        "\n",
        "    \n",
        "    normalised_states = np.append(normalised_states,normalised_board)\n",
        "\n",
        "\n",
        "  # print(\"states \",states)\n",
        "\n",
        "  normalised_states = normalised_states.reshape(-1,16)\n",
        "\n",
        "  # normalise next state\n",
        "  normalised_next_states = np.array([],dtype=np.float32)\n",
        "\n",
        "  for board in next_states:\n",
        "\n",
        "    normalised_board = np.array([],dtype=np.float32)\n",
        "\n",
        "    for tile in board:\n",
        "\n",
        "      if np.array_equal(tile,0):\n",
        "\n",
        "        normalised_board = np.append(normalised_board,0)\n",
        "\n",
        "      else:\n",
        "      \n",
        "        normalised_board = np.append(normalised_board,np.log2(tile)/7)\n",
        "\n",
        "\n",
        "    \n",
        "    normalised_next_states = np.append(normalised_next_states,normalised_board)\n",
        "\n",
        "  normalised_next_states = normalised_next_states.reshape(-1,16)\n",
        "\n",
        "\n",
        "  # normliase reward \n",
        "\n",
        "\n",
        "  # normliased_rewards = (rewards-1)/8\n",
        "  normalised_rewards = np.array([],dtype=np.float32)\n",
        "\n",
        "  for tile in rewards:\n",
        "\n",
        "    if tile == -1:\n",
        "\n",
        "      normalised_rewards = np.append(normalised_rewards,-1)\n",
        "\n",
        "    elif tile == 0:\n",
        "\n",
        "      normalised_rewards = np.append(normalised_rewards,0)\n",
        "\n",
        "    else:\n",
        "      \n",
        "      normalised_rewards = np.append(normalised_rewards,np.log2(tile)/7)\n",
        "\n",
        "  normalised_rewards = normalised_rewards.astype('float32')\n",
        "\n",
        "\n",
        "  return normalised_states,normalised_next_states,normalised_rewards\n",
        "\n",
        "def sample_experiences_per(batch_size):\n",
        "\tsample = replay_memory.sample(batch_size,beta)\n",
        "\tprint(\"beta \",beta) \n",
        "\t\n",
        "\tstates, actions, rewards, next_states, dones, weights, indexes = sample['obs'],sample['act'],sample['rew'],sample['next_obs'],sample['done'],sample['weights'],sample['indexes']\n",
        "  \n",
        "\tactions = np.reshape(actions,(batch_size,))\n",
        "\trewards = np.reshape(rewards,(batch_size,))\n",
        "\tdones = np.reshape(dones,(batch_size,))\n",
        "\t# weights = np.reshape(weights,(batch_size,-1))\n",
        "\n",
        "\n",
        "\treturn np.reshape(states,(batch_size,16)), actions, rewards, np.reshape(next_states,(batch_size,16)), dones, weights,indexes\n",
        "\n",
        "def sample_experiences(batch_size):\n",
        "\tsample = replay_memory.sample(batch_size)\n",
        "\tstates, actions, rewards, next_states, dones = sample['obs'],sample['act'],sample['rew'],sample['next_obs'],sample['done']\n",
        "    \n",
        "\tactions = np.reshape(actions,(batch_size,))\n",
        "\trewards = np.reshape(rewards,(batch_size,))\n",
        "\tdones = np.reshape(dones,(batch_size,))\n",
        "\n",
        "\n",
        "\treturn np.reshape(states,(batch_size,16)), actions, rewards, np.reshape(next_states,(batch_size,16)), dones\n",
        "\n",
        "def epsilon_greedy_policy(state, epsilon=0):\n",
        "\tif np.random.rand() < epsilon:\n",
        "\t\treturn np.random.randint(n_outputs)\n",
        "\telse:\n",
        "\n",
        "\t\t# normalise state\n",
        "\t\tnormalised_board = np.array([],dtype=np.float32)\n",
        "\n",
        "\t\tfor tile in state.flatten():\n",
        "\n",
        "\t\t\tif np.array_equal(tile,0):\n",
        "\n",
        "\t\t\t\tnormalised_board = np.append(normalised_board,0)\n",
        "\n",
        "\t\t\telse:\n",
        "      \n",
        "\t\t\t\tnormalised_board = np.append(normalised_board,np.log2(tile)/7)\n",
        "\t\t\n",
        "\t\tQ_values = model.predict(normalised_board[np.newaxis])\n",
        "  \n",
        "\n",
        "  \n",
        "\treturn np.argmax(Q_values[0])\n",
        "\n",
        "def play_one_step(env, state, epsilon,append_times,step_times,policy_times):\n",
        "\tstart_step_time = time.time()\n",
        "\n",
        "\n",
        "\tstart_policy_time = time.time()\n",
        "\taction = epsilon_greedy_policy(state, epsilon)\n",
        "\tend_policy_time = time.time()\n",
        "\n",
        "\tpolicy_times = np.append(policy_times,end_policy_time-start_policy_time)\n",
        "\n",
        "\tenv.make_move(action)\n",
        "\n",
        "\tenv.confirm_move()\n",
        "\n",
        "\tnext_state = env.get_board()\n",
        "\n",
        "\treward = env.get_move_score()\n",
        "\n",
        "\tdone, _ = env.verify_game_state()\n",
        " \n",
        "\tmerges = env.get_merges()\n",
        "\n",
        "\tstart_replay_buffer_time = time.time()\n",
        "\treplay_memory.add(obs=state,act=action,rew=reward,next_obs=next_state,done=done)\n",
        "\tend_replay_buffer_time = time.time()\n",
        "\n",
        "\tappend_times = np.append(append_times,end_replay_buffer_time-start_replay_buffer_time)\n",
        " \n",
        "\tend_step_time = time.time()\n",
        "\n",
        "\tstep_times = np.append(step_times,end_step_time-start_step_time)\n",
        "\n",
        "\treturn next_state, reward, done ,append_times,step_times,policy_times\n",
        "\n",
        "@tf.function(experimental_compile=True)\n",
        "def train_model_vanilla(states, actions, rewards, next_states, dones):\n",
        "\tnext_Q_values = model(next_states)\n",
        "\tmax_next_Q_values = tf.math.reduce_max(next_Q_values, axis=1)\n",
        "\ttarget_Q_values = (rewards + (1 - dones) * discount_rate * max_next_Q_values)\n",
        "\ttarget_Q_values = tf.reshape(target_Q_values,[-1,1])\n",
        "\tmask = tf.one_hot(actions, n_outputs)\n",
        "\twith tf.GradientTape() as tape:\n",
        "\t\tall_Q_values = model(states)\n",
        "\t\tQ_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "\t\tloss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "\tgrads = tape.gradient(loss, model.trainable_variables)\n",
        "\toptimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "@tf.function(experimental_compile=True)\n",
        "def train_model_double(states, actions, rewards, next_states, dones):\n",
        "  next_Q_values = model(next_states)    \n",
        "  best_next_actions = tf.math.argmax(next_Q_values,axis = 1)\n",
        "  next_mask = tf.one_hot(best_next_actions,n_outputs)\n",
        "  next_best_Q_values =  tf.math.reduce_sum(target(next_states) * next_mask,axis = 1)\n",
        "  target_Q_values = (rewards + (1 - dones) * discount_rate * next_best_Q_values)\n",
        "  target_Q_values = tf.reshape(target_Q_values,[-1,1])\n",
        "  mask = tf.one_hot(actions, n_outputs)\n",
        "  with tf.GradientTape() as tape:\n",
        "    all_Q_values = model(states)\n",
        "    Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "    loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "@tf.function(experimental_compile=True)\n",
        "def train_model_per(states, actions, rewards, next_states, dones,weights,indexes):\n",
        "\tnext_Q_values = model(next_states)\n",
        "\tmax_next_Q_values = tf.math.reduce_max(next_Q_values, axis=1)\n",
        "\ttarget_Q_values = (rewards + (1 - dones) * discount_rate * max_next_Q_values)\n",
        "\ttarget_Q_values = tf.reshape(target_Q_values,[-1,1])\n",
        "\tmask = tf.one_hot(actions, n_outputs)\n",
        "\twith tf.GradientTape() as tape:\n",
        "\t\tall_Q_values = model(states)\n",
        "\t\tQ_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "\t\tloss = tf.reduce_mean(loss_fn(target_Q_values, Q_values)*weights)\n",
        "\tgrads = tape.gradient(loss, model.trainable_variables)\n",
        "\toptimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\treturn tf.math.abs(target_Q_values - Q_values)\n",
        "\n",
        "@tf.function(experimental_compile=True)\n",
        "def train_model_double_per(states, actions, rewards, next_states, dones,weights,indexes):\n",
        "\tnext_Q_values = model(next_states)    \n",
        "\tbest_next_actions = tf.math.argmax(next_Q_values,axis = 1)\n",
        "\tnext_mask = tf.one_hot(best_next_actions,n_outputs)\n",
        "\tnext_best_Q_values =  tf.math.reduce_sum(target(next_states) * next_mask,axis = 1)\n",
        "\ttarget_Q_values = (rewards + (1 - dones) * discount_rate * next_best_Q_values)\n",
        "\ttarget_Q_values = tf.reshape(target_Q_values,[-1,1])\n",
        "\tmask = tf.one_hot(actions, n_outputs)\n",
        "\twith tf.GradientTape() as tape:\n",
        "\t\tall_Q_values = model(states)\n",
        "\t\tQ_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "\t\tloss = tf.reduce_mean(loss_fn(target_Q_values, Q_values)*weights)\n",
        "\tgrads = tape.gradient(loss, model.trainable_variables)\n",
        "\toptimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\treturn tf.math.abs(target_Q_values - Q_values)\n",
        "\n",
        "def training_step_per(batch_size):\n",
        "\ttraining_start = time.time()\n",
        "\n",
        "\tstart_replay_buffer_time = time.time()\n",
        "\texperiences = sample_experiences_per(batch_size)\n",
        "\tend_replay_buffer_time = time.time()\n",
        "\tstates, actions, rewards, next_states, dones, weights,indexes = experiences\n",
        "\n",
        "\n",
        "\tstates,next_states,rewards = normalise_training_samples(states,next_states,rewards)\n",
        "\t\n",
        "\t\n",
        "\n",
        "\tweights = train_model(states, actions, rewards, next_states, dones,weights,weights)\n",
        " \n",
        " \n",
        "  # adding the temporal difference error to the buffer\n",
        "\treplay_memory.update_priorities(indexes,weights)\n",
        "\n",
        "\tsample_time = end_replay_buffer_time-start_replay_buffer_time\n",
        "\n",
        "\ttraining_end = time.time()\n",
        "\n",
        "\ttraining_time = training_end-training_start\n",
        "\n",
        "\tstates,next_states,rewards = normalise_training_samples(states,next_states,rewards)\n",
        "\n",
        "\treturn sample_time,training_time\n",
        "\n",
        "def training_step_vanilla(batch_size):\n",
        "\ttraining_start = time.time()\n",
        "\n",
        "\tstart_replay_buffer_time = time.time()\n",
        "\texperiences = sample_experiences(batch_size)\n",
        "\tend_replay_buffer_time = time.time()\n",
        "\tstates, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "\t\n",
        "\n",
        "\tstates,next_states,rewards = normalise_training_samples(states,next_states,rewards)\n",
        "\n",
        "\t\n",
        "\t\n",
        "\ttrain_model(states, actions, rewards, next_states, dones)\n",
        "\n",
        "\tsample_time = end_replay_buffer_time-start_replay_buffer_time\n",
        "\n",
        "\ttraining_end = time.time()\n",
        "\n",
        "\ttraining_time = training_end-training_start\n",
        "\n",
        "\tstates,next_states,rewards = normalise_training_samples(states,next_states,rewards)\n",
        "\n",
        "\treturn sample_time,training_time\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MvZTmxO8iu1I",
        "outputId": "e243a1e4-c8a0-48d0-fdb0-cc8f5741bf9d"
      },
      "source": [
        "experiement_string = \"DDQN+D2QN+PER\"\n",
        "'''This is where the algorithm is decided for the experiement parameters\n",
        "Allowing experiement_strings are :\n",
        "\"DQN\"\n",
        "“DQN+PER”\n",
        "“DDQN”\n",
        "“DDQN+PER”\n",
        "“DDQN+D2QN”\n",
        "“DDQN+D2QN+PER”\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This is where the algorithm is decided for the experiement parameters\\nAllowing experiement_strings are :\\n\"DQN\"\\n“DQN+PER”\\n“DDQN”\\n“DDQN+PER”\\n“DDQN+D2QN”\\n“DDQN+D2QN+PER”\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbxTm5o0ijZ8"
      },
      "source": [
        "## **Set algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWmOcnvmhfxX"
      },
      "source": [
        "if experiement_string == \"DQN\":\n",
        "  training_step = training_step_vanilla\n",
        "  train_model = train_model_vanilla\n",
        "\n",
        "  learning_rate = 0.01\n",
        "  loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "elif experiement_string == \"DQN+PER\":\n",
        "  training_step = training_step_per\n",
        "  train_model = train_model_per\n",
        "\n",
        "  learning_rate = 0.01\n",
        "  loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "elif experiement_string == \"DDQN\":\n",
        "  training_step = training_step_vanilla\n",
        "  train_model = train_model_double\n",
        "\n",
        "  learning_rate = 0.006\n",
        "  loss_fn = keras.losses.Huber()\n",
        "\n",
        "elif experiement_string == \"DDQN+PER\":\n",
        "  training_step = training_step_per\n",
        "  train_model = train_model_double_per\n",
        "\n",
        "  learning_rate = 0.006\n",
        "  loss_fn = keras.losses.Huber()\n",
        "  \n",
        "elif experiement_string == \"DDQN+D2QN\":\n",
        "  training_step = training_step_vanilla\n",
        "  train_model = train_model_double\n",
        "\n",
        "  learning_rate = 0.0075\n",
        "  loss_fn = keras.losses.Huber()\n",
        "  \n",
        "elif experiement_string == \"DDQN+D2QN+PER\":\n",
        "  training_step = training_step_per\n",
        "  train_model = train_model_double_per\n",
        "\n",
        "  learning_rate = 0.0075\n",
        "  loss_fn = keras.losses.Huber()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDrrX2lJmF_F"
      },
      "source": [
        "## **Set transfer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1kIwYWZmV7r"
      },
      "source": [
        "directory = \"DSPFinal/Saves\"\n",
        "\n",
        "episodes = 600\n",
        "\n",
        "input_shape = [16]\n",
        "n_outputs = 4\n",
        "\n",
        "if not transfer:\n",
        "  if experiement_string == \"DQN\" or experiement_string == \"DQN+PER\" or experiement_string == \"DDQN\" or experiement_string ==  \"DDQN+PER\":\n",
        "    model = keras.models.Sequential([\n",
        "    keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
        "    keras.layers.Dense(32, activation=\"elu\"),\n",
        "    keras.layers.Dense(n_outputs)\n",
        "    ])\n",
        "   \n",
        "\n",
        "\n",
        "  elif experiement_string == \"DDQN+D2QN\" or experiement_string == \"DDQN+D2QN+PER\":\n",
        "    K = keras.backend\n",
        "    input_states = keras.layers.Input(shape=input_shape)\n",
        "    hidden1 = keras.layers.Dense(32, activation=\"elu\")(input_states)\n",
        "    hidden2 = keras.layers.Dense(32, activation=\"elu\")(hidden1)\n",
        "    state_values = keras.layers.Dense(1)(hidden2)\n",
        "    raw_advantages = keras.layers.Dense(n_outputs)(hidden2)\n",
        "    advantages = raw_advantages - K.max(raw_advantages, axis=1, keepdims=True)\n",
        "    Q_values = state_values + advantages\n",
        "    model = keras.models.Model(inputs=[input_states], outputs=[Q_values])\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "else: \n",
        "  \n",
        "\n",
        "  path = F\"/content/gdrive/My Drive/{directory}/{tranfser_file}model\" \n",
        "  old_model = keras.models.load_model(path)\n",
        "  if transfer_type == \"DQN→DQN\" :\n",
        "    old_model.summary()\n",
        "    layers = old_model.layers[0:2]\n",
        "    model = keras.models.Sequential()\n",
        "    for i in layers:\n",
        "      model.add(i)\n",
        "    model.add(keras.layers.Dense(4,name = \"dense_2\"))\n",
        "    model.summary()\n",
        "\n",
        "\n",
        "  elif transfer_type == \"D2QN→DQN\" :\n",
        "    old_model.summary()\n",
        "    layers = old_model.layers[0:3]\n",
        "    model = keras.models.Sequential()\n",
        "    for i in layers:\n",
        "      model.add(i)\n",
        "\n",
        "    model.add(keras.layers.Dense(4,name = \"dense_2\"))\n",
        "    model.summary()\n",
        "\n",
        "  elif transfer_type == \"DQN→D2QN\" :\n",
        "    old_model.summary()\n",
        "    K = keras.backend\n",
        "    input_states = keras.layers.Input(shape=[16])\n",
        "    hidden1 = keras.layers.Dense(32, activation=\"elu\")(input_states)\n",
        "    hidden2 = keras.layers.Dense(32, activation=\"elu\")(hidden1)\n",
        "    ouputs = keras.layers.Dense(4)(hidden2)\n",
        "    temp_model = keras.models.Model(inputs=[input_states], outputs=[ouputs])\n",
        "\n",
        "    temp_model.summary()\n",
        "\n",
        "    temp_model.set_weights(old_model.get_weights())\n",
        "\n",
        "    state_values = keras.layers.Dense(1)(temp_model.layers[-2].output)\n",
        "    raw_advantages = keras.layers.Dense(4)(hidden2)\n",
        "    advantages = raw_advantages - K.max(raw_advantages, axis=1, keepdims=True)\n",
        "    Q_values = state_values + advantages\n",
        "    model = keras.models.Model(inputs=[input_states], outputs=[Q_values])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "  elif transfer_type == \"D2QN→D2QN\" :\n",
        "    old_model.summary()\n",
        "\n",
        "    K = keras.backend\n",
        "    input_states = keras.layers.Input(shape=[16])\n",
        "    hidden1 = keras.layers.Dense(32, activation=\"elu\")(input_states)\n",
        "    hidden2 = keras.layers.Dense(32, activation=\"elu\")(hidden1)\n",
        "    ouputs = keras.layers.Dense(4)(hidden2)\n",
        "    temp_model = keras.models.Model(inputs=[input_states], outputs=[ouputs])\n",
        "    temp_model.summary()\n",
        "\n",
        "    layers = old_model.layers[0:3]\n",
        "    seq_model = keras.models.Sequential()\n",
        "    for i in layers:\n",
        "      seq_model.add(i)\n",
        "\n",
        "    seq_model.add(keras.layers.Dense(4,name = \"dense_2\"))\n",
        "\n",
        "    seq_model.summary()\n",
        "\n",
        "    temp_model.set_weights(seq_model.get_weights())\n",
        "\n",
        "    state_values = keras.layers.Dense(1)(temp_model.layers[-2].output)\n",
        "    raw_advantages = keras.layers.Dense(4)(hidden2)\n",
        "    advantages = raw_advantages - K.max(raw_advantages, axis=1, keepdims=True)\n",
        "    Q_values = state_values + advantages\n",
        "    model = keras.models.Model(inputs=[input_states], outputs=[Q_values])\n",
        "\n",
        "    model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdIhJA5ov_VQ"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVIllc5eo-02"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "general_stretergy = []\n",
        "\n",
        "general_stretergy.append([[[4,0,0,0],\n",
        "[0,0,2,0],\n",
        "[2,0,0,0],\n",
        "[2,0,0,0]], ['L','D'],\"General Stretergy Max Tile 4 A\"])\n",
        "\n",
        "general_stretergy.append([[[4,2,4,0],\n",
        "[0,0,2,0],\n",
        "[2,0,0,0],\n",
        "[2,0,0,4]], ['L','U','U'],\"General Stretergy Max Tile 4 B\"])\n",
        "\n",
        "general_stretergy.append([[[0,0,0,0],\n",
        "[0,2,0,0],\n",
        "[2,0,0,0],\n",
        "[4,0,0,0]], ['L','D','D'],\"General Stretergy Max Tile 4 C\"])\n",
        "\n",
        "general_stretergy.append([[[0,0,2,4],\n",
        "[0,2,0,0],\n",
        "[0,0,0,0],\n",
        "[0,0,0,2]], ['R','U','U'],\"General Stretergy Max Tile 4 D\"])\n",
        "\n",
        "general_stretergy.append([[[0,0,0,0],\n",
        "[0,2,0,0],\n",
        "[0,0,0,2],\n",
        "[0,2,0,4]]\n",
        ", ['D','R'],\"General Stretergy Max Tile 4 E\"])\n",
        "\n",
        "general_stretergy.append([[[0,0,0,0],\n",
        "[2,0,0,2],\n",
        "[2,0,0,0],\n",
        "[8,2,0,0]]\n",
        ", ['D','L'],\"General Stretergy Max Tile 8 A\"])\n",
        "\n",
        "general_stretergy.append([[[8,2,2,0],\n",
        "[4,2,2,0],\n",
        "[2,0,0,0],\n",
        "[0,0,0,0]]\n",
        ", ['L','L','U'],\"General Stretergy Max Tile 8 B\"])\n",
        "\n",
        "general_stretergy.append([[[0,0,0,0],\n",
        "[2,0,0,0],\n",
        "[4,0,0,0],\n",
        "[8,2,2,0]]\n",
        ", ['L'],\"General Stretergy Max Tile 8 C\"])\n",
        "\n",
        "general_stretergy.append([[[2,0,2,0],\n",
        "[2,0,0,0],\n",
        "[4,0,0,0],\n",
        "[8,4,0,0]]\n",
        ", ['D','D','D'],\"General Stretergy Max Tile 8 D\"])\n",
        "\n",
        "general_stretergy.append([[[0,0,0,2],\n",
        "[0,0,0,0],\n",
        "[4,2,0,0],\n",
        "[8,4,2,0]]\n",
        ", ['D','L','L','L'],\"General Stretergy Max Tile 8 E\"])\n",
        "\n",
        "general_stretergy.append([[[0,2,0,0],\n",
        "[4,0,0,0],\n",
        "[4,0,0,0],\n",
        "[16,4,0,0]]\n",
        ", ['D'],\"General Stretergy Max Tile 16 A\"])\n",
        "\n",
        "general_stretergy.append([[[0,0,0,0],\n",
        "[4,0,0,0],\n",
        "[8,4,2,0],\n",
        "[16,4,2,4]]\n",
        ", ['D','L'],\"General Stretergy Max Tile 16 B\"])\n",
        "\n",
        "general_stretergy.append([[[2,0,0,2],\n",
        "[0,0,0,0],\n",
        "[2,4,2,0],\n",
        "[16,8,2,4]]\n",
        ", ['D','L'],\"General Stretergy Max Tile 16 C\"])\n",
        "\n",
        "general_stretergy.append([[[2,2,2,0],\n",
        "[4,4,0,0],\n",
        "[8,8,2,0],\n",
        "[16,0,0,0]]\n",
        ", ['L','D'],\"General Stretergy Max Tile 16 D\"])\n",
        "\n",
        "general_stretergy.append([[[0,0,0,2],\n",
        "[2,0,0,0],\n",
        "[8,2,2,0],\n",
        "[16,4,4,2]]\n",
        ", ['L','D','D'],\"General Stretergy Max Tile 16 E\"])\n",
        "\n",
        "general_stretergy.append([[[0,0,0,2],\n",
        "[0,4,4,2],\n",
        "[0,2,8,2],\n",
        "[32,2,8,2]]\n",
        ", ['D','L'],\"General Stretergy Max Tile 32 A\"])\n",
        "\n",
        "general_stretergy.append([[[0,0,0,2],\n",
        "[2,0,2,2],\n",
        "[0,0,16,2],\n",
        "[32,16,2,8]] \n",
        ", ['D','L','D'],\"General Stretergy Max Tile 32 B\"])\n",
        "\n",
        "general_stretergy.append([[[0,2,0,2],\n",
        "[0,0,0,4],\n",
        "[0,4,4,2],\n",
        "[32,16,8,4]] \n",
        ", ['R','D','L','L'],\"General Stretergy Max Tile 32 C\"])\n",
        "\n",
        "general_stretergy.append([[[0,0,0,4],\n",
        "[0,0,0,4],\n",
        "[2,16,4,2],\n",
        "[32,16,8,2]] \n",
        ", ['D','L'],\"General Stretergy Max Tile 32 D\"])\n",
        "\n",
        "general_stretergy.append([[[0,0,0,4],\n",
        "[0,2,2,8],\n",
        "[16,8,4,4],\n",
        "[32,16,8,2]]\n",
        ", ['L','L','D','L'],\"General Stretergy Max Tile 32 E\"])\n",
        "\n",
        "general_stretergy.append([[[4,2,0,0],\n",
        "[8,0,0,0],\n",
        "[4,2,2,0],\n",
        "[64,4,8,2]]\n",
        ", ['L','D','L'],\"General Stretergy Max Tile 64 A\"])\n",
        "\n",
        "general_stretergy.append([[[4,2,4,2],\n",
        "[8,4,2,0],\n",
        "[4,8,4,0],\n",
        "[64,16,2,2]]\n",
        ", ['L','D'],\"General Stretergy Max Tile 64 B\"])\n",
        "\n",
        "general_stretergy.append([[[2,0,0,0],\n",
        "[8,4,0,4],\n",
        "[4,2,2,0],\n",
        "[64,4,64,0]]\n",
        ", ['L','D','D'],\"General Stretergy Max Tile 64 C\"])\n",
        "\n",
        "general_stretergy.append([[[4,0,4,0],\n",
        "[8,4,2,0],\n",
        "[16,8,8,8],\n",
        "[64,16,64,2]]\n",
        ", ['L','D','D'],\"General Stretergy Max Tile 64 D\"])\n",
        "\n",
        "general_stretergy.append([[[8,2,2,0],\n",
        "[4,8,4,0],\n",
        "[2,16,4,2],\n",
        "[64,32,16,8]]\n",
        ", ['L'],\"General Stretergy Max Tile 64 E\"])\n",
        "\n",
        "not_in_corner = []\n",
        "\n",
        "not_in_corner.append([[[2,0,0,0],\n",
        "[4,0,0,0],\n",
        "[0,0,0,0],\n",
        "[0,0,0,0]], ['D'],\"Not In Corner Max Tile 4 A\"])\n",
        "\n",
        "not_in_corner.append([[[2,0,0,0],\n",
        "[4,2,2,0],\n",
        "[0,0,0,0],\n",
        "[0,0,0,0]], ['D','L'],\"Not In Corner Max Tile 4 B\"])\n",
        "\n",
        "not_in_corner.append([[[0,0,0,0],\n",
        "[0,0,0,2],\n",
        "[4,0,0,0],\n",
        "[2,2,4,0]], ['L'],\"Not In Corner Max Tile 4 C\"])\n",
        "\n",
        "not_in_corner.append([[[2,4,2,0],\n",
        "[4,0,0,0],\n",
        "[8,0,2,0],\n",
        "[0,0,0,0]], ['D','L','L'],\"Not In Corner Max Tile 8 A\"])\n",
        "\n",
        "not_in_corner.append([[[4, 2, 0, 0],\n",
        "[8, 0, 0, 0],\n",
        "[4, 0, 0, 0],\n",
        "[2, 2, 0, 0]]\n",
        ", ['L','D','D'],\"Not In Corner Max Tile 8 B\"])\n",
        "\n",
        "not_in_corner.append([[[0,2,0,0],\n",
        "[4,0,0,0],\n",
        "[8,0,0,0],\n",
        "[2,2,0,2]], ['L','D','L','D'],\"Not In Corner Max Tile 8 C\"])\n",
        "\n",
        "not_in_corner.append([[[4,4,0,2],\n",
        "[16,0,0,0],\n",
        "[0,0,0,0],\n",
        "[0,0,0,2]], ['D'],\"Not In Corner Max Tile 16 A\"])\n",
        "\n",
        "not_in_corner.append([[[ 0,  0,  0,  2],\n",
        "[ 2,  0,  0,  2],\n",
        "[ 0,  0,  4,  2],\n",
        "[ 0,  0, 16,  2]], ['L','D'],\"Not In Corner Max Tile 16 B\"])\n",
        "\n",
        "not_in_corner.append([[[0,0,0,0],\n",
        "[0,0,2,0],\n",
        "[0,16,2,0],\n",
        "[8,8,4,2]], ['L','D'],\"Not In Corner Max Tile 16 C\"])\n",
        "\n",
        "not_in_corner.append([[[2,0,0,0],\n",
        "[2,2,0,0],\n",
        "[2,2,0,0],\n",
        "[4,32,8,2]], ['D','D'],\"Not In Corner Max Tile 32 A\"])\n",
        "\n",
        "not_in_corner.append([[[0,2,0,0],\n",
        "[4,0,0,0],\n",
        "[4,8,4,0],\n",
        "[16,32,16,4]], ['D','L','D','L'],\"Not In Corner Max Tile 32 B\"])\n",
        "\n",
        "not_in_corner.append([[[2,0,0,0],\n",
        "[4,2,0,0],\n",
        "[2,2,0,0],\n",
        "[4,32,8,2]], ['L','D'],\"Not In Corner Max Tile 32 C\"])\n",
        "\n",
        "not_in_corner.append([[[2,0,2,0],\n",
        "[4,2,0,0],\n",
        "[4,16,4,2],\n",
        "[4,64,2,4]], ['D','L'],\"Not In Corner Max Tile 64 A\"])\n",
        "\n",
        "not_in_corner.append([[[2,0,0,0],\n",
        "[2,16,0,2],\n",
        "[16,8,4,2],\n",
        "[32,64,8,4]], ['D','L','L','L','D','L'],\"Not In Corner Max Tile 64 B\"])\n",
        "\n",
        "not_in_corner.append([[[4,2,0,0],\n",
        "[4,4,4,0],\n",
        "[4,8,4,2],\n",
        "[32,64,8,4]], ['D','L'],\"Not In Corner Max Tile 64 C\"])\n",
        "\n",
        "no_immediate_reward = []\n",
        "\n",
        "no_immediate_reward.append([[[0,0,0,2],\n",
        "[2,0,0,0],\n",
        "[0,0,0,0],\n",
        "[4,2,0,0]], ['D'],\"No Immiedate Reward Tile 4 A\"])\n",
        "\n",
        "no_immediate_reward.append([[[0,0,0,0],\n",
        "[0,0,0,2],\n",
        "[0,0,0,0],\n",
        "[4,2,0,0]], ['D'],\"No Immiedate Reward Tile 4 B\"])\n",
        "\n",
        "no_immediate_reward.append([[[0,0,2,0],\n",
        "[0,0,0,0],\n",
        "[4,0,0,0],\n",
        "[8,2,0,0]], ['D'],\"No Immiedate Reward Tile 8 A\"])\n",
        "\n",
        "no_immediate_reward.append([[[2,0,0,0],\n",
        "[4,0,0,0],\n",
        "[2,0,0,0],\n",
        "[8,0,0,2]], ['L'],\"No Immiedate Reward Tile 8 B\"])\n",
        "\n",
        "stuck =[]\n",
        "\n",
        "stuck.append([[[0,0,0,0],\n",
        "[0,0,0,0],\n",
        "[2,0,0,0],\n",
        "[8,2,0,0]],\"Stuck Max Tile 8 A\"])\n",
        "\n",
        "stuck.append([[[0,0,0,0],\n",
        "[0,0,0,0],\n",
        "[0,0,0,0],\n",
        "[8,2,4,0]],\"Stuck Max Tile 8 B\"])\n",
        "\n",
        "stuck.append([[[0,2,0,0],\n",
        "[2,0,0,0],\n",
        "[8,2,0,0],\n",
        "[16,8,0,0]],\"Stuck Max Tile 16 A\"])\n",
        "\n",
        "stuck.append([[[16,4,0,0],\n",
        "[8,2,0,0],\n",
        "[4,0,0,0],\n",
        "[2,0,0,0]],\"Stuck Max Tile 16 B\"])\n",
        "\n",
        "stuck.append([[[0,0,0,0],\n",
        "[0,0,0,0],\n",
        "[4,2,4,2],\n",
        "[32,8,16,8]],\"Stuck Max Tile 32 A\"])\n",
        "\n",
        "stuck.append([[[2,0,0,0],\n",
        "[4,0,0,0],\n",
        "[16,4,2,0],\n",
        "[32,16,8,2]],\"Stuck Max Tile 32 B\"])\n",
        "\n",
        "stuck.append([[[4,2,0,0],\n",
        "[8,4,0,0],\n",
        "[4,8,4,0],\n",
        "[64,16,2,0]],\"Stuck Max Tile 64 A\"])\n",
        "\n",
        "stuck.append([[[2,8,2,0],\n",
        "[2,16,0,2],\n",
        "[2,8,4,2],\n",
        "[64,8,4,2]],\"Stuck Max Tile 64 B\"])\n",
        "\n",
        "board_flatten_size = 7\n",
        "\n",
        "def single_stuck_evaluation(model,env,start_state,seed):\n",
        "\n",
        "\n",
        "  tf.random.set_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  env.set_board(start_state)\n",
        "\n",
        "\n",
        "  state = env.get_board()\n",
        "\n",
        "  board_zero_best_tile = np.argmax(state)\n",
        "\n",
        "  action = epsilon_greedy_policy(state)\n",
        "\n",
        "  env.make_move(action)\n",
        "\n",
        "  env.confirm_move()\n",
        "\n",
        "\n",
        "  state = env.get_board()\n",
        "\n",
        "  board_one_best_tile = np.argmax(state)\n",
        "\n",
        "  action = epsilon_greedy_policy(state)\n",
        "\n",
        "  env.make_move(action)\n",
        "\n",
        "  env.confirm_move()\n",
        "\n",
        "  state = env.get_board()\n",
        "\n",
        "  board_two_best_tile = np.argmax(state)\n",
        "\n",
        "  in_corner_after_2_actions = 0\n",
        "\n",
        "  # if it moves tile away\n",
        "  if board_zero_best_tile != board_one_best_tile:\n",
        "    in_corner_after_2_actions += 0.5\n",
        "    # if it returns tile to orignal position\n",
        "    if board_zero_best_tile == board_two_best_tile:\n",
        "      in_corner_after_2_actions += 0.5\n",
        "\n",
        "  # print(\"board_zero_best_tile \",board_zero_best_tile)\n",
        "  # print(\"board_one_best_tile \",board_one_best_tile)\n",
        "  # print(\"board_two_best_tile \",board_two_best_tile)\n",
        "\n",
        "  # print(\"in_corner_after_2_actions\",in_corner_after_2_actions)\n",
        " \n",
        "\n",
        "  return in_corner_after_2_actions\n",
        "\n",
        " \n",
        "def rotate_stuck_evaluation(model,env,state,seed):\n",
        "\n",
        "  score_list = np.array([])\n",
        "\n",
        "  for i in range(4):\n",
        "    rotated_actions = np.array([])\n",
        "    \n",
        "    rotated_board = np.rot90(state, k=i, axes=(0, 1))\n",
        "\n",
        "    score = single_stuck_evaluation(model,env,rotated_board,seed)\n",
        "\n",
        "    score_list = np.append(score_list,score)\n",
        "\n",
        "    \n",
        "  #   print(\"rotated actions\",rotated_actions)\n",
        "  #   print(\"rotated board\",rotated_board)\n",
        "\n",
        "  # print(score_list)\n",
        "  return score_list\n",
        "\n",
        "def do_all_stuck_evaluation(evaluation,env,model): \n",
        "  table_data_all = {\"Evaluation\": [],\"Bottom Left\": [],\"Bottom Right\": [],\"Up Right\": [],\"Up Left\": []}\n",
        "\n",
        "  all_scores = np.array([])\n",
        "\n",
        "  for i in evaluation:\n",
        "    board = i[0]\n",
        "    label = i[1]\n",
        "\n",
        "    np_board = np.array(board,dtype=np.uint32)\n",
        "\n",
        "\n",
        "    # # rotating board so is in the left corner\n",
        "    # for k in range(4):\n",
        "    #   np_board_rotate = np.rot90(np_board, k=k, axes=(0, 1))\n",
        "    #   if np.argmax(np_board_rotate) == 12:\n",
        "    #     break\n",
        "\n",
        "\n",
        "    multiple_evaluation_score = np.array([])\n",
        "\n",
        "    # runs evaluation 10 times\n",
        "    for j in range(10):\n",
        "      evaluation_score = rotate_stuck_evaluation(model,env,np_board,j)\n",
        "      # print(\"evaluationScore\",evaluationScore)\n",
        "      multiple_evaluation_score = np.append(evaluation_score,multiple_evaluation_score)\n",
        "\n",
        "    multiple_evaluation_score = multiple_evaluation_score.reshape(-1,4)\n",
        "    # print(\"multipleEvaluationScore\",multipleEvaluationScore)\n",
        "\n",
        "    score = np.mean(multiple_evaluation_score,axis=0)\n",
        "\n",
        "    # print(label,\": \",score)\n",
        "    # sys.exit()\n",
        "\n",
        "    table_data_all['Evaluation'].append(label)\n",
        "\n",
        "    table_data_all['Bottom Left'].append(score[0])\n",
        "    table_data_all['Bottom Right'].append(score[1])\n",
        "    table_data_all['Up Right'].append(score[2])\n",
        "    table_data_all['Up Left'].append(score[3])\n",
        "\n",
        "    all_scores = np.append(all_scores,score)\n",
        "\n",
        "  mean_score = np.mean(all_scores)\n",
        "\n",
        "  shaped_score = all_scores.reshape(-1,4)\n",
        "\n",
        "  all_corner_means = np.mean(shaped_score,axis=0)\n",
        "\n",
        "  table_data_corner = {\"Bottom Left\": [all_corner_means[0]],\"Bottom Right\": [all_corner_means[1]],\"Up Right\": [all_corner_means[2]],\"Up Left\": [all_corner_means[3]]}\n",
        "\n",
        "  # print('allScore ',allScores)\n",
        "  # print('shapedScore ',shapedScore)\n",
        "  # print('allCornerMeans ',allCornerMeans)\n",
        "  # sys.exit()\n",
        "  print(\"Mean Score\",mean_score)\n",
        "  print(\"Corner %\")\n",
        "  print(tabulate(table_data_corner, headers='keys', tablefmt='fancy_grid'))\n",
        "  print(tabulate(table_data_all, headers='keys', tablefmt='fancy_grid'))\n",
        "\n",
        "\n",
        "def single_evaluation(model,env,state,actions,seed):\n",
        "\n",
        "  tf.random.set_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  model_actions = np.array([])\n",
        "\n",
        "  env.reset()\n",
        "  env.set_board(state)\n",
        "\n",
        "  obs = env.get_board()\n",
        "\n",
        "\n",
        "  # function 1 generating moves with the model\n",
        "  for i in range(actions.shape[0]):\n",
        "    normalised_board = np.array([],dtype=np.float32)\n",
        "\n",
        "    for tile in state.flatten():\n",
        "\n",
        "      if np.array_equal(tile,0):\n",
        "\n",
        "        normalised_board = np.append(normalised_board,0)\n",
        "\n",
        "      else:\n",
        "      \n",
        "        normalised_board = np.append(normalised_board,np.log2(tile)/board_flatten_size)\n",
        "    \n",
        "\n",
        "    Q_values = model.predict(normalised_board[np.newaxis])\n",
        "  \n",
        "    action = np.argmax(Q_values[0])\n",
        "\n",
        "\n",
        "    # adding model action to list\n",
        "    model_actions = np.append(model_actions,action)\n",
        "\n",
        "    env.make_move(action)\n",
        "\n",
        "    env.confirm_move()\n",
        "\n",
        "    state = env.get_board()\n",
        "\n",
        "  # function 2 comparing the 2 lists \n",
        "\n",
        "  correct_moves = 0\n",
        "\n",
        "  for i in range(actions.shape[0]):\n",
        "    if actions[i] == model_actions[i]:\n",
        "      correct_moves += 1\n",
        "\n",
        "  score = correct_moves / actions.shape[0]\n",
        "\n",
        "  \n",
        "  # print(\"\")\n",
        "  # print(\"Model Actions \",model_actions)\n",
        "  # print(\"Correct moves \",correct_moves)\n",
        "  # print(\"Score\",score)\n",
        "  \n",
        "  return score \n",
        "\n",
        "def all_rotate_evaluation(model,env,state,actions,seed):\n",
        "\n",
        "  score_list = np.array([])\n",
        "\n",
        "  for i in range(4):\n",
        "    rotated_actions = np.array([])\n",
        "    if i == 0:\n",
        "      rotated_actions = np.append(rotated_actions,actions)\n",
        "    elif i == 1:\n",
        "      for a in range(actions.shape[0]):\n",
        "        if actions[a] == 0:\n",
        "          rotated_actions = np.append(rotated_actions,3)\n",
        "        elif actions[a] == 2:\n",
        "          rotated_actions = np.append(rotated_actions,1)\n",
        "        elif actions[a] == 1:\n",
        "          rotated_actions = np.append(rotated_actions,0)\n",
        "        elif actions[a] == 3:\n",
        "          rotated_actions = np.append(rotated_actions,2)\n",
        "    elif i == 2:\n",
        "      for a in range(actions.shape[0]):\n",
        "        if actions[a] == 0:\n",
        "          rotated_actions = np.append(rotated_actions,2)\n",
        "        elif actions[a] == 2:\n",
        "          rotated_actions = np.append(rotated_actions,0)\n",
        "        elif actions[a] == 1:\n",
        "          rotated_actions = np.append(rotated_actions,3)\n",
        "        elif actions[a] == 3:\n",
        "          rotated_actions = np.append(rotated_actions,1)\n",
        "    elif i == 3:\n",
        "      for a in range(actions.shape[0]):\n",
        "        if actions[a] == 0:\n",
        "          rotated_actions = np.append(rotated_actions,1)\n",
        "        elif actions[a] == 2:\n",
        "          rotated_actions = np.append(rotated_actions,3)\n",
        "        elif actions[a] == 1:\n",
        "          rotated_actions = np.append(rotated_actions,2)\n",
        "        elif actions[a] == 3:\n",
        "          rotated_actions = np.append(rotated_actions,0)\n",
        "\n",
        "    rotated_board = np.rot90(state, k=i, axes=(0, 1))\n",
        "\n",
        "    score = single_evaluation(model,env,rotated_board,rotated_actions,seed)\n",
        "\n",
        "    score_list = np.append(score_list,score)\n",
        "\n",
        "    \n",
        "    # print(\"rotated actions\",rotated_actions)\n",
        "    # print(\"rotated board\",rotated_board)\n",
        "\n",
        "  # print(score_list)\n",
        "  return score_list\n",
        "\n",
        "def do_evaluations(general_stretergy,model):\n",
        "\n",
        "  table_data_all = {\"Evaluation\": [],\"Bottom Left\": [],\"Bottom Right\": [],\"Up Right\": [],\"Up Left\": []}\n",
        "\n",
        "  all_scores = np.array([])\n",
        "\n",
        "  for g in general_stretergy:\n",
        "    board = g[0]\n",
        "    moves = g[1]\n",
        "    label = g[2]\n",
        "\n",
        "\n",
        "    np_board = np.array(board,dtype=np.uint32)\n",
        "\n",
        "\n",
        "    # rotating board so is in the left corner\n",
        "    for k in range(4):\n",
        "      np_board_rotate = np.rot90(np_board, k=k, axes=(0, 1))\n",
        "      if np.argmax(np_board_rotate) == 12:\n",
        "        break\n",
        "\n",
        "    \n",
        "\n",
        "    np_moves = np.array([])\n",
        "    for i in moves:\n",
        "      if i == 'L':\n",
        "        np_moves = np.append(np_moves,0)\n",
        "      elif i == 'U':\n",
        "        np_moves = np.append(np_moves,1)\n",
        "      elif i == 'R':\n",
        "        np_moves = np.append(np_moves,2)\n",
        "      elif i == 'D':\n",
        "        np_moves = np.append(np_moves,3)\n",
        "\n",
        "\n",
        "    multiple_evaluation_score = np.array([])\n",
        "\n",
        "    # runs evaluation 10 times\n",
        "    for i in range(10):\n",
        "      evaluation_score = all_rotate_evaluation(model,env,np_board_rotate,np_moves,i)\n",
        "      # print(\"evaluationScore\",evaluationScore)\n",
        "      multiple_evaluation_score = np.append(evaluation_score,multiple_evaluation_score)\n",
        "\n",
        "    multiple_evaluation_score = multiple_evaluation_score.reshape(-1,4)\n",
        "    # print(\"multipleEvaluationScore\",multipleEvaluationScore)\n",
        "\n",
        "    score = np.mean(multiple_evaluation_score,axis=0)\n",
        "\n",
        "    # print(label,\": \",score)\n",
        "    # sys.exit()\n",
        "\n",
        "    table_data_all['Evaluation'].append(label)\n",
        "\n",
        "    table_data_all['Bottom Left'].append(score[0])\n",
        "    table_data_all['Bottom Right'].append(score[1])\n",
        "    table_data_all['Up Right'].append(score[2])\n",
        "    table_data_all['Up Left'].append(score[3])\n",
        "\n",
        "    all_scores = np.append(all_scores,score)\n",
        "\n",
        "  mean_score = np.mean(all_scores)\n",
        "\n",
        "  shaped_score = all_scores.reshape(-1,4)\n",
        "\n",
        "  all_corner_means = np.mean(shaped_score,axis=0)\n",
        "\n",
        "  table_data_corner = {\"Bottom Left\": [all_corner_means[0]],\"Bottom Right\": [all_corner_means[1]],\"Up Right\": [all_corner_means[2]],\"Up Left\": [all_corner_means[3]]}\n",
        "\n",
        "  # print('allScore ',allScores)\n",
        "  # print('shapedScore ',shapedScore)\n",
        "  # print('allCornerMeans ',allCornerMeans)\n",
        "  # sys.exit()\n",
        "  print(\"Mean Score\",mean_score)\n",
        "  print(\"Corner %\")\n",
        "  print(tabulate(table_data_corner, headers='keys', tablefmt='fancy_grid'))\n",
        "  print(tabulate(table_data_all, headers='keys', tablefmt='fancy_grid'))\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "def do_not_in_corner_evaluations(general_stretergy,model):\n",
        "\n",
        "  table_data_all = {\"Evaluations\": [],\"Bottom Left\": [],\"Bottom Right\": [],\"Up Right\": [],\"Up Left\": []}\n",
        "\n",
        "  all_scores = np.array([])\n",
        "\n",
        "  for g in general_stretergy:\n",
        "    board = g[0]\n",
        "    moves = g[1]\n",
        "    label = g[2]\n",
        "\n",
        "\n",
        "    np_board = np.array(board,dtype=np.uint32)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    np_moves = np.array([])\n",
        "    for i in moves:\n",
        "      if i == 'L':\n",
        "        np_moves = np.append(np_moves,0)\n",
        "      elif i == 'U':\n",
        "        np_moves = np.append(np_moves,1)\n",
        "      elif i == 'R':\n",
        "        np_moves = np.append(np_moves,2)\n",
        "      elif i == 'D':\n",
        "        np_moves = np.append(np_moves,3)\n",
        "\n",
        "\n",
        "    multiple_evaluation_score = np.array([])\n",
        "\n",
        "    # runs evaluation 10 times\n",
        "    for i in range(10):\n",
        "      evaluation_score = all_rotate_evaluation(model,env,np_board,np_moves,i)\n",
        "      # print(\"evaluationScore\",evaluationScore)\n",
        "      multiple_evaluation_score = np.append(evaluation_score,multiple_evaluation_score)\n",
        "\n",
        "    multiple_evaluation_score = multiple_evaluation_score.reshape(-1,4)\n",
        "    # print(\"multipleEvaluationScore\",multipleEvaluationScore)\n",
        "\n",
        "    score = np.mean(multiple_evaluation_score,axis=0)\n",
        "\n",
        "    # print(label,\": \",score)\n",
        "    # sys.exit()\n",
        "\n",
        "    table_data_all['Evaluation'].append(label)\n",
        "\n",
        "    table_data_all['Bottom Left'].append(score[0])\n",
        "    table_data_all['Bottom Right'].append(score[1])\n",
        "    table_data_all['Up Right'].append(score[2])\n",
        "    table_data_all['Up Left'].append(score[3])\n",
        "\n",
        "    all_scores = np.append(all_scores,score)\n",
        "\n",
        "  mean_score = np.mean(all_scores)\n",
        "\n",
        "  shaped_score = all_scores.reshape(-1,4)\n",
        "\n",
        "  all_corner_means = np.mean(shaped_score,axis=0)\n",
        "\n",
        "  table_data_corner = {\"Bottom Left\": [all_corner_means[0]],\"Bottom Right\": [all_corner_means[1]],\"Up Right\": [all_corner_means[2]],\"Up Left\": [all_corner_means[3]]}\n",
        "\n",
        "  # print('allScore ',allScores)\n",
        "  # print('shapedScore ',shapedScore)\n",
        "  # print('allCornerMeans ',allCornerMeans)\n",
        "  # sys.exit()\n",
        "  print(\"Mean Score\",mean_score)\n",
        "  print(\"Corner %\")\n",
        "  print(tabulate(table_data_corner, headers='keys', tablefmt='fancy_grid'))\n",
        "  print(tabulate(table_data_all, headers='keys', tablefmt='fancy_grid'))\n",
        "\n",
        "\n",
        "\n",
        "#Converting observations in range (0,1) using log(n)/log(max) so that gradients don't vanish\n",
        "def process_log(observation):\n",
        "        observation = np.reshape(observation, (4, 4))\n",
        "        observation_temp = np.where(observation <= 0, 1, observation) \n",
        "        processed_observation = np.log2(observation_temp)/np.log2(65536)\n",
        "        return processed_observation.reshape(1,4,4)\n",
        "\n",
        "def do_hundrad_twenty_eight_evaluation_dsgiitr(model,env):\n",
        "  best_tile_list = np.array([])\n",
        "  list_of_episodes = np.array([])\n",
        "  \n",
        "\n",
        "  for i in range(do_hundrad_twenty_eight_evaluation_num):\n",
        "\n",
        "    tf.random.set_seed(i)\n",
        "    np.random.seed(i)\n",
        "\n",
        "    env.reset()\n",
        "\n",
        "    state = env.get_board()\n",
        "\n",
        "    done, _ = env.verify_game_state()\n",
        "\n",
        "    while not done:\n",
        "      # print(\"\")\n",
        "      # print(\"State \",state)\n",
        "\n",
        "      action = epsilon_greedy_policy(state)\n",
        "\n",
        "      # print(\"Action \",action)\n",
        "\n",
        "      env.make_move(action)\n",
        "\n",
        "      env.confirm_move()\n",
        "\n",
        "      state = env.get_board()\n",
        "\n",
        "      done, _ = env.verify_game_state()\n",
        "\n",
        "      # print(\"Next State \",state)\n",
        "      if np.max(state) == 128:\n",
        "        break \n",
        "\n",
        "    list_of_episodes = np.append(list_of_episodes,i)\n",
        "\n",
        "    best_tile_pos = np.argmax(state)\n",
        "    best_tile = np.max(state)\n",
        "    best_tile_list = np.append(best_tile_list,best_tile)\n",
        "\n",
        "  if save_max_tiles:\n",
        "    path = F\"/content/gdrive/My Drive/{directory}/{save_name}maxtiles.p\"\n",
        "    with open(path, 'wb') as f:\n",
        "      pickle.dump(store_obj, f) \n",
        "\n",
        "  list_of_episodes = list_of_episodes +1\n",
        "\n",
        "  mpl.style.use('seaborn')\n",
        "  plt.plot(list_of_episodes,best_tile_list)\n",
        "\n",
        "  plt.xlabel(\"Episodes\")\n",
        "\n",
        "  plt.ylabel(\"Best Tile\")\n",
        "\n",
        "  plt.show()  \n",
        "\n",
        "  twos = 0\n",
        "  fours = 0\n",
        "  eights = 0\n",
        "  sixteens = 0\n",
        "  thirtytwos = 0\n",
        "  sixtiyfours = 0\n",
        "  hundradtweighteights = 0\n",
        "\n",
        "\n",
        "  for i in best_tile_list:\n",
        "    if i == 2:\n",
        "      twos += 1\n",
        "    elif i==4:\n",
        "      fours += 1\n",
        "    elif i==8:\n",
        "      eights += 1\n",
        "    elif i==16:\n",
        "      sixteens += 1\n",
        "    elif i==32:\n",
        "      thirtytwos += 1\n",
        "    elif i==64:\n",
        "      sixtiyfours += 1\n",
        "    elif i==128:\n",
        "      hundradtweighteights += 1\n",
        "\n",
        "\n",
        "  table_data_all = {\"2\": [twos],\"4\": [fours],\"8\": [eights],\"16\": [sixteens],\"32\": [thirtytwos],\"64\": [sixtiyfours],\"128\": [hundradtweighteights]}\n",
        "\n",
        "  print(tabulate(table_data_all, headers='keys', tablefmt='fancy_grid'))\n",
        "\n",
        "\n",
        "def do_hundrad_twenty_eight_evaluation(model,env):\n",
        "  best_tile_list = np.array([])\n",
        "  list_of_episodes = np.array([])\n",
        "  \n",
        "\n",
        "  for i in range(do_hundrad_twenty_eight_evaluation_num):\n",
        "\n",
        "    tf.random.set_seed(i)\n",
        "    np.random.seed(i)\n",
        "\n",
        "    env.reset()\n",
        "\n",
        "    state = env.get_board()\n",
        "\n",
        "    done, _ = env.verify_game_state()\n",
        "\n",
        "    while not done:\n",
        "      # print(\"\")\n",
        "      # print(\"State \",state)\n",
        "\n",
        "      action = epsilon_greedy_policy(state)\n",
        "\n",
        "      # print(\"Action \",action)\n",
        "\n",
        "      env.make_move(action)\n",
        "\n",
        "      env.confirm_move()\n",
        "\n",
        "      state = env.get_board()\n",
        "\n",
        "      done, _ = env.verify_game_state()\n",
        "\n",
        "      # print(\"Next State \",state)\n",
        "      if np.max(state) == 128:\n",
        "        break \n",
        "\n",
        "    list_of_episodes = np.append(list_of_episodes,i)\n",
        "\n",
        "    best_tile_pos = np.argmax(state)\n",
        "    best_tile = np.max(state)\n",
        "    best_tile_list = np.append(best_tile_list,best_tile)\n",
        "\n",
        "  if save_max_tiles:\n",
        "    path = F\"/content/gdrive/My Drive/{directory}/{save_name}maxtiles.p\"\n",
        "    with open(path, 'wb') as f:\n",
        "      pickle.dump(store_obj, f) \n",
        "\n",
        "  list_of_episodes = list_of_episodes +1\n",
        "\n",
        "  mpl.style.use('seaborn')\n",
        "  plt.plot(list_of_episodes,best_tile_list)\n",
        "\n",
        "  plt.xlabel(\"Episodes\")\n",
        "\n",
        "  plt.ylabel(\"Best Tile\")\n",
        "\n",
        "  plt.show()  \n",
        "\n",
        "  twos = 0\n",
        "  fours = 0\n",
        "  eights = 0\n",
        "  sixteens = 0\n",
        "  thirtytwos = 0\n",
        "  sixtiyfours = 0\n",
        "  hundradtweighteights = 0\n",
        "\n",
        "\n",
        "  for i in best_tile_list:\n",
        "    if i == 2:\n",
        "      twos += 1\n",
        "    elif i==4:\n",
        "      fours += 1\n",
        "    elif i==8:\n",
        "      eights += 1\n",
        "    elif i==16:\n",
        "      sixteens += 1\n",
        "    elif i==32:\n",
        "      thirtytwos += 1\n",
        "    elif i==64:\n",
        "      sixtiyfours += 1\n",
        "    elif i==128:\n",
        "      hundradtweighteights += 1\n",
        "\n",
        "\n",
        "  table_data_all = {\"2\": [twos],\"4\": [fours],\"8\": [eights],\"16\": [sixteens],\"32\": [thirtytwos],\"64\": [sixtiyfours],\"128\": [hundradtweighteights]}\n",
        "\n",
        "  print(tabulate(table_data_all, headers='keys', tablefmt='fancy_grid'))\n",
        "  \n",
        "\n",
        "\n",
        "def learning_graph():\n",
        "\n",
        "\n",
        "  epsilon = 1\n",
        "  episodes_e_reaches_min = 500\n",
        "  min_epsilon = 0.01\n",
        "  e_decay = (epsilon-min_epsilon)/episodes_e_reaches_min\n",
        "\n",
        "  list_of_epsilon = np.array([])\n",
        "  list_of_episodes = np.array([])\n",
        "\n",
        "  for i in range(episodes):\n",
        "    epsilon = max(epsilon-e_decay,0.01)\n",
        "    list_of_epsilon = np.append(list_of_epsilon,epsilon)\n",
        "    list_of_episodes = np.append(list_of_episodes,i)\n",
        "\n",
        "  norm_list_of_epsilon = list_of_epsilon * 256\n",
        "\n",
        "  list_of_episodes = list_of_episodes +1\n",
        "\n",
        "\n",
        "  mpl.style.use('seaborn')\n",
        "  \n",
        "  # plot lines\n",
        "\n",
        "  plt.plot(list_of_episodes,list_of_board_sum, label = \"Score (Board sum)\")\n",
        "  plt.plot(list_of_episodes,list_of_tiles ,label = \"Best Tile\")\n",
        "  plt.plot(list_of_episodes,norm_list_of_epsilon,  label = \"Epsilon (Normalised)\")\n",
        "\n",
        "  plt.xlabel(\"Episodes\")\n",
        "\n",
        "  plt.ylabel(\"Score\")\n",
        "\n",
        "  plt.title(save_name)\n",
        "\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def epsilon_greedy_policy(state, epsilon=0):\n",
        "\tif np.random.rand() < epsilon:\n",
        "\t\treturn np.random.randint(n_outputs)\n",
        "\telse:\n",
        "\n",
        "\t\t# normalise state\n",
        "\t\tnormalised_board = np.array([],dtype=np.float32)\n",
        "\n",
        "\t\tfor tile in state.flatten():\n",
        "\n",
        "\t\t\tif np.array_equal(tile,0):\n",
        "\n",
        "\t\t\t\tnormalised_board = np.append(normalised_board,0)\n",
        "\n",
        "\t\t\telse:\n",
        "      \n",
        "\t\t\t\tnormalised_board = np.append(normalised_board,np.log2(tile)/7)\n",
        "\t\t\n",
        "\t\tQ_values = model.predict(normalised_board[np.newaxis])\n",
        "  \n",
        "\n",
        "  \n",
        "\treturn np.argmax(Q_values[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJt6lhVJiAF9"
      },
      "source": [
        "## **Numba game logic**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOou6Nzch9Nc"
      },
      "source": [
        "# training and game logic cells\n",
        "\n",
        "spec = [\n",
        "    (\"__board_size\", int32),\n",
        "    (\"__total_score\", int32),\n",
        "    (\"__score\", int32),\n",
        "    (\"__merges\", int32),\n",
        "    (\"__temp_board\", uint32[:, :]),\n",
        "    (\"__board\", uint32[:, :]),\n",
        "    (\"__done_merge\", b1),\n",
        "    (\"__done_cover_up\", b1),\n",
        "    (\"__invalid_count\", uint32),\n",
        "    (\"__total_count\", uint32),\n",
        "    (\"__invalid_move_warmup\", uint32),\n",
        "    (\"__invalid_move_threshold\", float64),\n",
        "    (\"__power_mat\", uint32[:, :, :]),\n",
        "    (\"__penalty\", int32),\n",
        "]\n",
        "\n",
        "\n",
        "@jitclass(spec)\n",
        "class Game2048:\n",
        "    def __init__(self, board_size: int, invalid_move_warmup=16, invalid_move_threshold=0.1, penalty=-512):\n",
        "        \"\"\"\n",
        "        This class is responsible to implement the game. \n",
        "        Parameters\n",
        "        ----------\n",
        "        board_size : int\n",
        "            Size of the board. Default=4\n",
        "        invalid_move_warmup : int\n",
        "            Minimum of invalid movements to finish the episode. Default=16\n",
        "        invalid_move_threshold : float\n",
        "            How much(fraction) invalid movements is necessary according to the total of moviments already executed. to finish the episode after invalid_move_warmup. Default 0.1 \n",
        "        penalty : int\n",
        "            Penalization of invalid movements to sum up in reward function. Default=-512\n",
        "        \"\"\"\n",
        "\n",
        "        self.__board_size = board_size\n",
        "        self.__score = 0\n",
        "        self.__merges = 0\n",
        "        self.__total_score = 0\n",
        "        self.__invalid_count = 0\n",
        "        self.__total_count = 0\n",
        "        self.__invalid_move_warmup = invalid_move_warmup\n",
        "        self.__invalid_move_threshold = invalid_move_threshold\n",
        "        self.__penalty = penalty\n",
        "        self.__board = np.zeros((board_size, board_size), dtype=np.uint32)\n",
        "        self.__temp_board = np.zeros((board_size, board_size), dtype=np.uint32)\n",
        "        self.__add_two_or_four()\n",
        "        self.__add_two_or_four()\n",
        "        self.__power_mat = np.zeros((board_size, board_size, 16 + (board_size - 4)), dtype=np.uint32)\n",
        "\n",
        "    def __add_two_or_four(self):\n",
        "        \"\"\"Add tile with number two.\"\"\"\n",
        "\n",
        "        indexes = np.where(self.__board == 0)\n",
        "\n",
        "        if len(indexes[0]) == 0:\n",
        "            return\n",
        "\n",
        "        # Coordinates to add a tile with number two\n",
        "        index = np.random.choice(np.arange(len(indexes[0])))\n",
        "\n",
        "        if np.random.uniform(0, 1) >= 0.9:\n",
        "            self.__board[indexes[0][index]][indexes[1][index]] = 4\n",
        "        else:\n",
        "            self.__board[indexes[0][index]][indexes[1][index]] = 2\n",
        "\n",
        "    def __transpose(self, board):\n",
        "        \"\"\"Transpose a matrix.\"\"\"\n",
        "\n",
        "        temp = np.zeros((self.__board_size, self.__board_size), dtype=np.uint32)\n",
        "\n",
        "        for line in range(self.__board_size):\n",
        "            for column in range(self.__board_size):\n",
        "                temp[column][line] = board[line][column]\n",
        "\n",
        "        return temp\n",
        "\n",
        "    def __reverse(self, board):\n",
        "        \"\"\"Reverse a matrix.\"\"\"\n",
        "\n",
        "        temp = np.zeros((self.__board_size, self.__board_size), dtype=np.uint32)\n",
        "\n",
        "        for line in range(self.__board_size):\n",
        "            for column in range(self.__board_size):\n",
        "                temp[line][column] = board[self.__board_size - line - 1][column]\n",
        "\n",
        "        return temp\n",
        "\n",
        "    def __cover_up(self, board):\n",
        "        \"\"\"Cover the most antecedent zeros with non-zero number. \"\"\"\n",
        "\n",
        "        temp = np.zeros((self.__board_size, self.__board_size), dtype=np.uint32)\n",
        "        self.__done_cover_up = False\n",
        "\n",
        "        for column in range(self.__board_size):\n",
        "            up = 0\n",
        "            for line in range(self.__board_size):\n",
        "                if board[line][column] != 0:\n",
        "                    temp[up][column] = board[line][column]\n",
        "                    up = up + 1\n",
        "                    if up != line:\n",
        "                        self.__done_cover_up = True\n",
        "\n",
        "        return temp\n",
        "\n",
        "    def __merge(self, board):\n",
        "        \"\"\"Verify if a merge is possible and execute.\"\"\"\n",
        "\n",
        "        self.__done_merge = False\n",
        "\n",
        "        for line in range(1, self.__board_size):\n",
        "            for column in range(self.__board_size):\n",
        "                if board[line][column] == board[line - 1][column]:\n",
        "\n",
        "                    if board[line][column] > 0:\n",
        "                      self.__merges += 1\n",
        "\n",
        "                    self.__score = self.__score + (board[line][column] * 2)\n",
        "                    board[line - 1][column] = board[line - 1][column] * 2\n",
        "                    board[line][column] = 0\n",
        "                    self.__done_merge = True\n",
        "\n",
        "                    \n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "        return board\n",
        "\n",
        "    def __up(self):\n",
        "\n",
        "        temp = self.__cover_up(self.__board)\n",
        "        temp = self.__merge(temp)\n",
        "        temp = self.__cover_up(temp)\n",
        "        self.__temp_board = temp\n",
        "\n",
        "    def __down(self):\n",
        "\n",
        "        temp = self.__reverse(self.__board)\n",
        "        temp = self.__merge(temp)\n",
        "        temp = self.__cover_up(temp)\n",
        "        temp = self.__reverse(temp)\n",
        "        self.__temp_board = temp\n",
        "\n",
        "    def __right(self):\n",
        "\n",
        "        temp = self.__reverse(self.__transpose(self.__board))\n",
        "        temp = self.__merge(temp)\n",
        "        temp = self.__cover_up(temp)\n",
        "        temp = self.__transpose(self.__reverse(temp))\n",
        "        self.__temp_board = temp\n",
        "\n",
        "    def __left(self):\n",
        "\n",
        "        temp = self.__transpose(self.__board)\n",
        "        temp = self.__merge(temp)\n",
        "        temp = self.__cover_up(temp)\n",
        "        temp = self.__transpose(temp)\n",
        "        self.__temp_board = temp\n",
        "\n",
        "    def get_merges(self):\n",
        "\n",
        "      return self.__merges\n",
        "\n",
        "    def get_move_score(self):\n",
        "        \"\"\"Get the last score move.\"\"\"\n",
        "\n",
        "        return self.__score\n",
        "\n",
        "    def get_total_score(self):\n",
        "        \"\"\"Get the total score gained until now.\"\"\"\n",
        "\n",
        "        return self.__total_score\n",
        "\n",
        "    def set_board(self, board):\n",
        "        \"\"\"This function is only for test purpose.\"\"\"\n",
        "\n",
        "        self.__board = board\n",
        "\n",
        "    def get_board(self):\n",
        "        \"\"\"Get the actual board.\"\"\"\n",
        "\n",
        "        return self.__board\n",
        "\n",
        "    def confirm_move(self):\n",
        "        \"\"\"Execute movement.\"\"\"\n",
        "        self.__total_count = self.__total_count + 1\n",
        "        self.__total_score = self.__total_score + self.__score\n",
        "        if np.array_equal(self.__board, self.__temp_board):\n",
        "            self.__invalid_count = self.__invalid_count + 1\n",
        "            self.__score = self.__penalty\n",
        "        else:\n",
        "            self.__board = self.__temp_board.copy()\n",
        "            self.__add_two_or_four()\n",
        "\n",
        "    def make_move(self, move):\n",
        "        \"\"\"Make a move.\"\"\"\n",
        "        self.__score = 0\n",
        "        self.__merges = 0\n",
        "\n",
        "        if move == 1:\n",
        "            self.__up()\n",
        "        if move == 3:\n",
        "            self.__down()\n",
        "        if move == 2:\n",
        "            self.__right()\n",
        "        if move == 0:\n",
        "            self.__left()\n",
        "\n",
        "    def verify_game_state(self):\n",
        "        \"Check if the game has done or not.\"\n",
        "        if (\n",
        "            self.__invalid_count > self.__invalid_move_warmup\n",
        "            and self.__invalid_count > self.__invalid_move_threshold * self.__total_count\n",
        "        ):\n",
        "            return True, self.__penalty\n",
        "\n",
        "        # Verify zero entries\n",
        "        for line in range(self.__board_size):\n",
        "            for column in range(self.__board_size):\n",
        "                if self.__board[line][column] == 0:\n",
        "                    return False, 0\n",
        "\n",
        "        # Verify possible merges\n",
        "        for line in range(1, self.__board_size):\n",
        "            for column in range(1, self.__board_size):\n",
        "                if (\n",
        "                    self.__board[line][column] == self.__board[line][column - 1]\n",
        "                    or self.__board[line][column] == self.__board[line - 1][column]\n",
        "                ):\n",
        "                    return False, 0\n",
        "\n",
        "        # Veirfy possible merges in first column and first line\n",
        "        for line in range(1, self.__board_size):\n",
        "            if self.__board[line][0] == self.__board[line - 1][0]:\n",
        "                return False, 0\n",
        "\n",
        "        for column in range(1, self.__board_size):\n",
        "            if self.__board[0][column] == self.__board[0][column - 1]:\n",
        "                return False, 0\n",
        "\n",
        "        return True, self.__penalty\n",
        "\n",
        "    def get_power_2_mat(self):\n",
        "        \"Get power 2 matrix.\"\n",
        "        return self.__power_mat\n",
        "\n",
        "    def transform_board_to_power_2_mat(self):\n",
        "        \"Transform board to a power 2 matrix.\"\n",
        "        self.__power_mat = np.zeros(\n",
        "            shape=(self.__board_size, self.__board_size, 16 + (self.__board_size - 4)), dtype=np.uint32\n",
        "        )\n",
        "\n",
        "        for line in range(self.__board_size):\n",
        "            for column in range(self.__board_size):\n",
        "                if self.__board[line][column] == 0:\n",
        "                    self.__power_mat[line][column][0] = 1\n",
        "                else:\n",
        "                    power = int(np.log2(self.__board[line][column]))\n",
        "                    self.__power_mat[line][column][power] = 1\n",
        "\n",
        "    def reset(self):\n",
        "        \"Reset the game.\"\n",
        "        self.__board = np.zeros((self.__board_size, self.__board_size), dtype=np.uint32)\n",
        "        self.__temp_board = np.zeros((self.__board_size, self.__board_size), dtype=np.uint32)\n",
        "        self.__score = 0\n",
        "        self.__total_score = 0\n",
        "        self.__invalid_count = 0\n",
        "        self.__total_count = 0\n",
        "        self.__add_two_or_four()\n",
        "        self.__add_two_or_four()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNkk7RhmtF7Q"
      },
      "source": [
        "## **Main loop**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyfw-l6V5Wim"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkSanL5_hnOJ"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\tdrive.mount('/content/gdrive')\n",
        "\tprint(\" Started:\",ctime())\n",
        " \n",
        "\tbeta = 0.4\n",
        "\tepisodes_b_reaches_max = 600\n",
        "\tb_increase = (1-beta)/episodes_b_reaches_max\n",
        "\n",
        "\tkeras.backend.clear_session()\n",
        "\ttf.random.set_seed(42)\n",
        "\tnp.random.seed(42)\n",
        "\n",
        "\tenv = Game2048(board_size = 4,penalty = 0)\n",
        "\tinput_shape = [16] # == env.observation_space.shape\n",
        "\tn_outputs = 4 # == env.action_space.n\n",
        "  \n",
        "\n",
        " \n",
        "\ttarget = keras.models.clone_model(model) \n",
        "\ttarget.set_weights(model.get_weights())\n",
        "\n",
        "\n",
        "\tlist_of_steps = np.array([])\n",
        "\tlist_of_tiles = np.array([])\n",
        "\tlist_of_board_sum = np.array([])\n",
        "\tlist_of_score = np.array([])\n",
        "  \n",
        "\n",
        "\tunqiue_run_num = str(int(time.time()))\n",
        "\tprint(\"Unique Run Number : \",unqiue_run_num)\n",
        "\n",
        "\tenv_spec = {'rew': {'shape': 1, 'dtype': np.float32},\n",
        "             'done': {'shape': 1, 'dtype': np.float32},\n",
        "             'obs': {'dtype': np.float32, 'shape': (4, 4)},\n",
        "             'next_obs': {'dtype': np.float32, 'shape': (4, 4)}, \n",
        "             'act': {'dtype': np.int32, 'shape': 1}}\n",
        "\n",
        "\tmemory_len = 2000\n",
        "\n",
        "\t# replay_memory = PrioritizedReplayBuffer(memory_len,env_spec)\n",
        "\treplay_memory = PrioritizedReplayBuffer(memory_len,env_spec)\n",
        "\treplay_memory.clear()\n",
        "\n",
        "\tbatch_size = 32\n",
        "\tdiscount_rate = 0.95\n",
        "\tlearning_rate = 6e-3\n",
        "\n",
        "\toptimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\tloss_fn = keras.losses.Huber()\n",
        "\t# loss_fn = importance_weighted_mse\n",
        "\n",
        "\t# env.seed(42)\n",
        "\tnp.random.seed(42)\n",
        "\ttf.random.set_seed(42)\n",
        "\n",
        "\n",
        "\n",
        "\t# Epsilon parameters\n",
        "\tepsilon = 1\n",
        "\tepisodes_e_reaches_min = 500\n",
        "\tmin_epsilon = 0.01\n",
        "\te_decay = (epsilon-min_epsilon)/episodes_e_reaches_min\n",
        "\n",
        "\t# rewards = [] \n",
        "\tbest_score = 0\n",
        "\n",
        "\n",
        "\n",
        "\tfor episode in range(episodes):\n",
        "   \n",
        "\t\tepisode_start = time.time()\n",
        "\t\t\n",
        "\t\tappend_times = np.array([])\n",
        "\n",
        "\t\tstep_times = np.array([])\n",
        "\n",
        "\t\tpolicy_times = np.array([])\n",
        "\t\t\n",
        "\t\tenv.reset() \n",
        "  \n",
        "\t\tobs = env.get_board()\n",
        "\n",
        "\t\tdone = False  \n",
        "\t\t\n",
        "\t\tstep = 0\n",
        "\n",
        "\t\tscore = 0\n",
        "\n",
        "\t\twhile not done:\n",
        "\n",
        "\n",
        "\t\t\tobs, reward, done, append_times,step_times,policy_times = play_one_step(env, obs, epsilon,append_times,step_times,policy_times)\n",
        "\n",
        "\t\t\tstep += 1\n",
        "\n",
        "\t\t\tif np.max(obs) == 128:\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\tscore += reward\n",
        "\t\t\t\n",
        "\t\t# rewards.append(step) # Not shown in the book\n",
        "\n",
        "\n",
        "\t\t# update epsilon\n",
        "\t\tepsilon = epsilon = max(epsilon-e_decay,min_epsilon)\n",
        "  \n",
        "    # update beta\n",
        "\t\tbeta = min(beta+b_increase,1)\n",
        "\n",
        "\t\tepisode_end = time.time()\n",
        "\n",
        "\t\tepisode_div_steps = (episode_end-episode_start)/step\n",
        "\n",
        "\t\t# if episode == 1:\n",
        "\t\t# \t\tsys.exit()\n",
        "\n",
        "\t\tlist_of_steps = np.append(list_of_steps,step)\n",
        "\t\tlist_of_tiles= np.append(list_of_tiles,np.max(obs))\n",
        "\t\tlist_of_score = np.append(list_of_score,score)\n",
        "\t\tlist_of_board_sum = np.append(list_of_board_sum,np.sum(obs))\n",
        "\t\t\n",
        "\n",
        "\t\tprint(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\") \n",
        "\t\tprint(\" Policy Time Total \",np.sum(policy_times),\" Policy Time Mean \",np.mean(policy_times),end=\"\")\n",
        "\t\tprint(\" Append Time Total \",np.sum(append_times),\" Append Time Mean \",np.mean(append_times),end=\"\")\n",
        "\t\tprint(\" Total Step time \",np.sum(step_times),\" Mean Step Time \",np.mean(step_times),end =\"\")\n",
        "\t\tprint(\" Mean Episode Time \",episode_div_steps,\" Total Episode Time \",episode_end-episode_start,end=\"\")\n",
        "\t\tprint(\" Best Tile \",np.max(obs),\" Score (board sum)\", np.sum(obs))\n",
        "\n",
        "   \n",
        "\t\t# if (episode % 32)/32 == 0 and episode > 1:\n",
        "\n",
        "     # train every episode after 50\n",
        "\t\tif episode > 50:\n",
        "\t\t\tsample_time,training_time = training_step(batch_size)\n",
        "\t\t\tprint(\" Sample time \",sample_time,end=\"\")\n",
        "\t\t\tprint(\" Training time \",training_time)\n",
        "\n",
        "\n",
        "    # copying online model to target model\n",
        "\t\tif episode % 50 == 0: \n",
        "\t\t\t\ttarget.set_weights(model.get_weights())\n",
        "    \n",
        "\t\tprint(\"Episode \", episode,\" Loop Completed at \", ctime())\n",
        "\n",
        "      \n",
        "\tprint(\" Ended:\",ctime())\n",
        " \n",
        "  # store run time data\n",
        "\tstore_obj = pickle_class(episodes,list_of_steps,list_of_tiles,list_of_board_sum,list_of_score)\n",
        "\tpath = F\"/content/gdrive/My Drive/{directory}/{save_name}saveobj.p\"\n",
        "\twith open(path, 'wb') as f:\n",
        "\t\tpickle.dump(store_obj, f) \n",
        "\n",
        "\n",
        " \n",
        "  # save model \n",
        "\tpath = F\"/content/gdrive/My Drive/{directory}/{save_name}model\" \n",
        "\tmodel.save(path,save_format='h5')\n",
        "\tprint(\"model saved as \",unqiue_run_num,\".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFRMzk2sfvNn"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTOo_xibfyS5"
      },
      "source": [
        "#Training graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81hI6bJYfxx2"
      },
      "source": [
        "learning_graph()\n",
        "print(ctime())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7TWXaxn7mJf",
        "outputId": "846b1d97-761a-4a1b-91ce-9e1f71584dcc"
      },
      "source": [
        "'''Connect to drive'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjLPUWxF8bkA"
      },
      "source": [
        "env = Game2048(board_size = 4,penalty = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8vs2e_k7wgG"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsZ-p6tR7kCz"
      },
      "source": [
        "path = F\"/content/gdrive/My Drive/trained_model.model\" \n",
        "old_model = keras.models.load_model(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPWjIiEg7JfP"
      },
      "source": [
        "model = old_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jBtXxJd75q2",
        "outputId": "bb436818-0f37-47c8-b540-8a0a0b9c1819"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_3 (Flatten)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1024)              17408     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 4)                 1028      \n",
            "=================================================================\n",
            "Total params: 674,564\n",
            "Trainable params: 674,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K2TIagjs7_b"
      },
      "source": [
        "#General evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuSsoih1tKWW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "defb0f08-c695-40a7-8b4d-a77d02c178a5"
      },
      "source": [
        "board_flatten_size = 7 #since log2(128<-the goal)/7 = 1\n",
        "\n",
        "do_evaluations(general_stretergy,model)\n",
        "\n",
        "print(ctime())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 4, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 4, 4), dtype=tf.float32, name='flatten_3_input'), name='flatten_3_input', description=\"created by layer 'flatten_3_input'\"), but it was called on an input with incompatible shape (None, 16).\n",
            "Mean Score 0.25\n",
            "Corner %\n",
            "╒═══════════════╤════════════════╤════════════╤═══════════╕\n",
            "│   Bottom Left │   Bottom Right │   Up Right │   Up Left │\n",
            "╞═══════════════╪════════════════╪════════════╪═══════════╡\n",
            "│     0.0433333 │       0.436667 │   0.453333 │ 0.0666667 │\n",
            "╘═══════════════╧════════════════╧════════════╧═══════════╛\n",
            "╒═════════════════════════════════╤═══════════════╤════════════════╤════════════╤═══════════╕\n",
            "│ Evaluation                      │   Bottom Left │   Bottom Right │   Up Right │   Up Left │\n",
            "╞═════════════════════════════════╪═══════════════╪════════════════╪════════════╪═══════════╡\n",
            "│ General Stretergy Max Tile 4 A  │      0        │       0.5      │   0.5      │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 4 B  │      0        │       0        │   0.333333 │  0.666667 │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 4 C  │      0        │       0.666667 │   0.333333 │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 4 D  │      0.333333 │       0        │   0        │  0.666667 │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 4 E  │      0.5      │       0.5      │   0        │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 8 A  │      0        │       0.5      │   0.5      │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 8 B  │      0        │       0        │   0.666667 │  0.333333 │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 8 C  │      0        │       0        │   1        │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 8 D  │      0        │       1        │   0        │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 8 E  │      0        │       0.25     │   0.75     │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 16 A │      0        │       1        │   0        │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 16 B │      0        │       0.5      │   0.5      │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 16 C │      0        │       0.5      │   0.5      │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 16 D │      0        │       0.5      │   0.5      │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 16 E │      0        │       0.666667 │   0.333333 │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 32 A │      0        │       0.5      │   0.5      │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 32 B │      0        │       0.666667 │   0.333333 │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 32 C │      0.25     │       0.25     │   0.5      │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 32 D │      0        │       0.5      │   0.5      │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 32 E │      0        │       0.25     │   0.75     │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 64 A │      0        │       0.333333 │   0.666667 │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 64 B │      0        │       0.5      │   0.5      │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 64 C │      0        │       0.666667 │   0.333333 │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 64 D │      0        │       0.666667 │   0.333333 │  0        │\n",
            "├─────────────────────────────────┼───────────────┼────────────────┼────────────┼───────────┤\n",
            "│ General Stretergy Max Tile 64 E │      0        │       0        │   1        │  0        │\n",
            "╘═════════════════════════════════╧═══════════════╧════════════════╧════════════╧═══════════╛\n",
            "Tue Aug 10 21:24:41 2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyGgbs4CGCZT"
      },
      "source": [
        "# Not in corner evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPqGVUfBGGKb"
      },
      "source": [
        "do_evaluations(not_in_corner,model)\n",
        "\n",
        "print(ctime())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9Ota5c8GHPc"
      },
      "source": [
        "# Correct direction evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh5Utz7WGHFM"
      },
      "source": [
        "do_evaluations(no_immediate_reward,model)\n",
        "\n",
        "print(ctime())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tAyfDMnGoc3"
      },
      "source": [
        "# Do stuck evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyyAI-FpGuUq"
      },
      "source": [
        "do_all_stuck_evaluation(stuck,env,model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0uJ7bgU5Ym2"
      },
      "source": [
        "# 128 tile consistency evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjOu7kNUs3_2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "9136bb77-f66d-4a16-daec-b04bd481a52d"
      },
      "source": [
        "do_hundrad_twenty_eight_evaluation(model,env)\n",
        "\n",
        "print(ctime())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "╒═════╤═════╤═════╤══════╤══════╤══════╤═══════╕\n",
            "│   2 │   4 │   8 │   16 │   32 │   64 │   128 │\n",
            "╞═════╪═════╪═════╪══════╪══════╪══════╪═══════╡\n",
            "│   5 │  10 │  10 │    0 │    0 │    0 │     0 │\n",
            "╘═════╧═════╧═════╧══════╧══════╧══════╧═══════╛\n",
            "Tue Aug 10 21:29:25 2021\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}